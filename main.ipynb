{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Symbol, MatrixSymbol, Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Symbol(\"l\")\n",
    "d_model = Symbol(r\"d_{model}\")\n",
    "d_ff = Symbol(r\"d_{ff}\")\n",
    "n_vocab = Symbol(r\"n_{vocab}\")\n",
    "n_layers = Symbol(r\"n_{layers}\")\n",
    "n_heads = Symbol(r\"n_{heads}\")\n",
    "\n",
    "inputs = MatrixSymbol(\"\\text{inputs}\", 1, l)\n",
    "\n",
    "softmax = Function(r\"\\text{softmax}\")\n",
    "relu = Function(r\"\\text{relu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    @property\n",
    "    def flops_dict(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def flops_count(self):\n",
    "        def count(d):\n",
    "            return sum([count(v) if isinstance(v, dict) else v for v in d.values()])\n",
    "        return count(self.flops_dict)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(d, n):\n",
    "    return {k: repeat(v, n) if isinstance(v, dict) else n*v for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(Layer):    \n",
    "    def __init__(self, l, d_model, n_vocab):\n",
    "        super().__init__()\n",
    "        self.l, self.d_model, self.n_vocab = l, d_model, n_vocab\n",
    "\n",
    "    def evaluate(self):\n",
    "        X_word = MatrixSymbol(r\"X_{word}\", self.l, self.d_model)\n",
    "        X_pos = MatrixSymbol(r\"X_{pos}\", self.l, self.d_model)\n",
    "\n",
    "        return X_word + X_pos\n",
    "\n",
    "    @property\n",
    "    def flops_dict(self):\n",
    "        return {\n",
    "            \"X_word + X_pos\": self.l * self.d_model,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle d_{model} l$"
      ],
      "text/plain": [
       "d_{model}*l"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embeddings(l, d_model, n_vocab).flops_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self, m, n):\n",
    "        super().__init__()\n",
    "        self.m, self.n = m, n\n",
    "\n",
    "    @property\n",
    "    def flops_dict(self):\n",
    "        return {\n",
    "            \"x - max(x)\": self.m * self.n,\n",
    "            \"e^x\": self.m * self.n,\n",
    "            \"sum\": self.m * self.n,\n",
    "        }\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(Layer):\n",
    "    def __init__(self, n_q, n_k, d_k, d_v):\n",
    "        super().__init__()\n",
    "        self.n_q, self.n_k, self.d_k, self.d_v = n_q, n_k, d_k, d_v\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Q = MatrixSymbol(\"Q\", self.n_q, self.d_k)\n",
    "        # K = MatrixSymbol(\"K\", self.n_k, self.d_k)\n",
    "        # V = MatrixSymbol(\"Q\", self.n_k, self.d_v)\n",
    "        # mask = MatrixSymbol(\"Q\", self.n_q, self.n_k)\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def flops_dict(self):\n",
    "        return {\n",
    "            \"Q * K.T\": 2 * self.n_q * self.d_k * self.n_k,\n",
    "            \"/ sqrt(d_k)\": self.n_q * self.n_k,\n",
    "            \"+ mask\": self.n_q * self.n_k,\n",
    "            \"Softmax\": Softmax(self.n_q, self.n_k).flops_dict,\n",
    "            \"* V\": 2 * self.n_q * self.n_k * self.d_v,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 4 d_{model} l^{2} + 5 l^{2}$"
      ],
      "text/plain": [
       "4*d_{model}*l**2 + 5*l**2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ScaledDotProductAttention(l, l, d_model, d_model).flops_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, n_q, n_k, d_k, d_v, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_q = n_q\n",
    "        self.n_k = n_k\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "    \n",
    "    def evaluate(self):\n",
    "        # WQ = MatrixSymbol(\"WQ\", d_model, d_k * n_heads)\n",
    "        # WK = MatrixSymbol(\"WK\", d_model, d_k * n_heads)\n",
    "        # WV = MatrixSymbol(\"WV\", d_model, d_v * n_heads)\n",
    "        # WO = MatrixSymbol(\"WO\", d_v * n_heads, d_model)\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def flops_dict(self):\n",
    "        # the FLOPs count is the same no matter the number of heads, the heads just \n",
    "        # enable different representation subspaces, it doesn't add any computational \n",
    "        # complexity as far as FLOPs is concerned\n",
    "        return {\n",
    "            \"Q * WQ\": 2 * self.n_q * self.d_model * (self.d_k * self.n_heads),\n",
    "            \"K * WK\": 2 * self.n_k * self.d_model * (self.d_k * self.n_heads),\n",
    "            \"V * WV\": 2 * self.n_k * self.d_model * (self.d_v * self.n_heads),\n",
    "            \"ScaledDotProductAttention\": repeat(ScaledDotProductAttention(self.n_q, self.n_k, self.d_k, self.d_v).flops_dict, self.n_heads),\n",
    "            \"* WO\": 2 * self.n_q * (self.d_v * self.n_heads) * self.d_model\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 8 d_{model}^{2} l n_{heads} + 4 d_{model} l^{2} n_{heads} + 5 l^{2} n_{heads}$"
      ],
      "text/plain": [
       "8*d_{model}**2*l*n_{heads} + 4*d_{model}*l**2*n_{heads} + 5*l**2*n_{heads}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadAttention(l, l, d_model, d_model, d_model, n_heads).flops_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-Wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(Layer):\n",
    "    def __init__(self, l, d_in, d_ff, d_out):\n",
    "        super().__init__()\n",
    "        self.l, self.d_in, self.d_ff, self.d_out = l, d_in, d_ff, d_out\n",
    "    \n",
    "    def evaluate(self):\n",
    "        # W1 = MatrixSymbol(\"W1\", d_in, d_ff)\n",
    "        # b1 = MatrixSymbol(\"b1\", d_ff, 1)\n",
    "\n",
    "        # W1 = MatrixSymbol(\"W1\", d_ff, d_out)\n",
    "        # b1 = MatrixSymbol(\"b1\", d_out, 1)\n",
    "\n",
    "        # X = MatrixSymbol(\"X\", l, d_in)\n",
    "        # output = relu(X * W1 + b1) * W2 + b2\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def flops_dict(self):\n",
    "        return {\n",
    "            \"* W1\": 2 * self.l * self.d_in * self.d_ff,\n",
    "            \"+ b1\": self.d_ff ,\n",
    "            \"* W2\": 2 * self.l * self.d_ff * self.d_out,\n",
    "            \"+ b2\": self.d_out,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 4 d_{ff} d_{model} l + d_{ff} + d_{model}$"
      ],
      "text/plain": [
       "4*d_{ff}*d_{model}*l + d_{ff} + d_{model}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PositionWiseFFN(l, d_model, d_ff, d_model).flops_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(Layer):\n",
    "    def __init__(self, m, n):\n",
    "        super().__init__()\n",
    "        self.m, self.n = m, n\n",
    "    \n",
    "    def evaluate(self):\n",
    "        # X = MatrixSymbol(\"X\", n, m)\n",
    "        #\n",
    "        # mean_x = sum(X, axis=-1) / m\n",
    "        # var_x = sum(X**2, axis=-1) / m - mean_x**2\n",
    "        #\n",
    "        # numer = x - mean_x\n",
    "        # denom = sqrt(var_x + eps)\n",
    "        #\n",
    "        # gamma * (numer / denom) + beta\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def flops_dict(self):\n",
    "        n, m = self.n, self.m\n",
    "        return {\n",
    "            \"mean(x)\": m*n + n, # element wise add and then a divide over the resulting vector\n",
    "            \"var_x\": { \n",
    "                \"X**2\": m*n,\n",
    "                \"sum\": m*n,\n",
    "                \"/ m\": n,\n",
    "                \"mean_x**2\": n,\n",
    "                \"m - mean_x**2\": n\n",
    "            },\n",
    "            \"numerator\": n, # x - mean_x\n",
    "            \"denominator\": 2*n, # sqrt(var_x + eps), elem wise addition then elem wise sqrt\n",
    "            \"gamma*(numerator/denominator)+beta\": 3*m*n # after broadcasting, 1 elem wise divide, multiply, then add\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 6 d_{model} l + 7 d_{model}$"
      ],
      "text/plain": [
       "6*d_{model}*l + 7*d_{model}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LayerNorm(l, d_model).flops_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(Layer):\n",
    "    def __init__(self, l, d_model, d_ff, n_heads):\n",
    "        super().__init__()\n",
    "        self.l, self.d_model, self.d_ff, self.n_heads = l, d_model, d_ff, n_heads\n",
    "    \n",
    "    def evaluate(self):\n",
    "        # X = MatrixSymbol(\"X\", l, d_model)\n",
    "        # X = X + MultiHeadAttention(LayerNorm(X))\n",
    "        # X = X + PositionWiseFFN(LayerNorm(X))\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def flops_dict(self):\n",
    "        return {\n",
    "            \"LayerNorms\": repeat(LayerNorm(self.l, self.d_model).flops_dict, 2),\n",
    "            \"ResidualConnections\": 2 * self.l * self.d_model,\n",
    "            \"MultiHeadAttention\": MultiHeadAttention(self.l, self.l, self.d_model / self.n_heads, self.d_model / self.n_heads, self.d_model, self.n_heads).flops_dict,\n",
    "            \"PositionWiseFFN\": PositionWiseFFN(self.l, self.d_model, self.d_ff, self.d_model).flops_dict\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 4 d_{ff} d_{model} l + d_{ff} + 8 d_{model}^{2} l + 4 d_{model} l^{2} + 14 d_{model} l + 15 d_{model} + 5 l^{2} n_{heads}$"
      ],
      "text/plain": [
       "4*d_{ff}*d_{model}*l + d_{ff} + 8*d_{model}**2*l + 4*d_{model}*l**2 + 14*d_{model}*l + 15*d_{model} + 5*l**2*n_{heads}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Block(l, d_model, d_ff, n_heads).flops_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Layer):\n",
    "    def __init__(self, l, d_model, d_ff, n_vocab, n_layers, n_heads):\n",
    "        super().__init__()\n",
    "        self.l, self.d_model, self.d_ff, self.n_vocab, self.n_layers, self.n_heads = l, d_model, d_ff, n_vocab,n_layers,  n_heads\n",
    "    \n",
    "    def evaluate(self):\n",
    "        # X = embeddings\n",
    "        # for _ in range(n_layers):\n",
    "        #   X = block(X)\n",
    "        # X = layer_norm(X)\n",
    "        # W_lm = MatrixSymbol(\"LM\", d_model, n_vocab) # projection_to_vocab (lm head)\n",
    "        # outputs = X * W_lm\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def flops_dict(self):\n",
    "        return {\n",
    "            \"Embeddings\": Embeddings(self.l, self.d_model, self.n_vocab).flops_dict,\n",
    "            \"LayerNorm_Final\": LayerNorm(self.l, self.d_model).flops_dict,\n",
    "            \"ProjectToVocab\": 2 * self.l * self.d_model * self.n_vocab,\n",
    "            \"Blocks\": repeat(Block(self.l, self.d_model, self.d_ff, self.n_heads).flops_dict, self.n_layers)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 4 d_{ff} d_{model} l n_{layers} + d_{ff} n_{layers} + 8 d_{model}^{2} l n_{layers} + 4 d_{model} l^{2} n_{layers} + 12 d_{model} l n_{layers} + 2 d_{model} l n_{vocab} + 7 d_{model} l + 13 d_{model} n_{layers} + 7 d_{model} + 5 l^{2} n_{heads} n_{layers} + n_{layers} \\left(2 d_{model} l + 2 d_{model}\\right)$"
      ],
      "text/plain": [
       "4*d_{ff}*d_{model}*l*n_{layers} + d_{ff}*n_{layers} + 8*d_{model}**2*l*n_{layers} + 4*d_{model}*l**2*n_{layers} + 12*d_{model}*l*n_{layers} + 2*d_{model}*l*n_{vocab} + 7*d_{model}*l + 13*d_{model}*n_{layers} + 7*d_{model} + 5*l**2*n_{heads}*n_{layers} + n_{layers}*(2*d_{model}*l + 2*d_{model})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops = Transformer(l, d_model, d_ff, n_vocab, n_layers, n_heads).flops_count\n",
    "flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 8 d^{2} l n + 4 d f l n + 4 d l^{2} n + 12 d l n + 2 d l v + 7 d l + 13 d n + 7 d + f n + 5 h l^{2} n + n \\left(2 d l + 2 d\\right)$"
      ],
      "text/plain": [
       "8*d**2*l*n + 4*d*f*l*n + 4*d*l**2*n + 12*d*l*n + 2*d*l*v + 7*d*l + 13*d*n + 7*d + f*n + 5*h*l**2*n + n*(2*d*l + 2*d)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops = flops.subs(d_model, Symbol(\"d\"))\n",
    "flops = flops.subs(d_ff, Symbol(\"f\"))\n",
    "flops = flops.subs(n_layers, Symbol(\"n\"))\n",
    "flops = flops.subs(n_vocab, Symbol(\"v\"))\n",
    "flops = flops.subs(n_heads, Symbol(\"h\"))\n",
    "flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 13 d n + 7 d + f n + l^{2} \\cdot \\left(4 d n + 5 h n\\right) + l \\left(8 d^{2} n + 4 d f n + 12 d n + 2 d v + 7 d\\right) + n \\left(2 d l + 2 d\\right)$"
      ],
      "text/plain": [
       "13*d*n + 7*d + f*n + l**2*(4*d*n + 5*h*n) + l*(8*d**2*n + 4*d*f*n + 12*d*n + 2*d*v + 7*d) + n*(2*d*l + 2*d)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops.collect(\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 8 d^{2} l n + d \\left(4 f l n + 4 l^{2} n + 12 l n + 2 l v + 7 l + n \\left(2 l + 2\\right) + 13 n + 7\\right) + f n + 5 h l^{2} n$"
      ],
      "text/plain": [
       "8*d**2*l*n + d*(4*f*l*n + 4*l**2*n + 12*l*n + 2*l*v + 7*l + n*(2*l + 2) + 13*n + 7) + f*n + 5*h*l**2*n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops.collect(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 8 d^{2} l n + 4 d l^{2} n + 12 d l n + 2 d l v + 7 d l + 13 d n + 7 d + f \\left(4 d l n + n\\right) + 5 h l^{2} n + n \\left(2 d l + 2 d\\right)$"
      ],
      "text/plain": [
       "8*d**2*l*n + 4*d*l**2*n + 12*d*l*n + 2*d*l*v + 7*d*l + 13*d*n + 7*d + f*(4*d*l*n + n) + 5*h*l**2*n + n*(2*d*l + 2*d)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops.collect(\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 8 d^{2} l n + 4 d f l n + 4 d l^{2} n + 12 d l n + 2 d l v + 7 d l + 13 d n + 7 d + f n + 5 h l^{2} n + n \\left(2 d l + 2 d\\right)$"
      ],
      "text/plain": [
       "8*d**2*l*n + 4*d*f*l*n + 4*d*l**2*n + 12*d*l*n + 2*d*l*v + 7*d*l + 13*d*n + 7*d + f*n + 5*h*l**2*n + n*(2*d*l + 2*d)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops.collect(\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 8 d^{2} l n + 4 d f l n + 4 d l^{2} n + 12 d l n + 2 d l v + 7 d l + 13 d n + 7 d + f n + 5 h l^{2} n + n \\left(2 d l + 2 d\\right)$"
      ],
      "text/plain": [
       "8*d**2*l*n + 4*d*f*l*n + 4*d*l**2*n + 12*d*l*n + 2*d*l*v + 7*d*l + 13*d*n + 7*d + f*n + 5*h*l**2*n + n*(2*d*l + 2*d)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops.collect(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51248964096"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_flops(N, l, d_model, d_ff, n_vocab, n_layers, n_heads):\n",
    "    return int(N*Transformer(l, d_model, d_ff, n_vocab, n_layers, n_heads).flops_count)\n",
    "\n",
    "count_flops(\n",
    "    N=1,\n",
    "    l=512,\n",
    "    d_model=512,\n",
    "    d_ff=4096,\n",
    "    n_vocab=30000,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Embeddings': {'X_word + X_pos': 262144},\n",
       " 'LayerNorm_Final': {'mean(x)': 262656,\n",
       "  'var_x': {'X**2': 262144,\n",
       "   'sum': 262144,\n",
       "   '/ m': 512,\n",
       "   'mean_x**2': 512,\n",
       "   'm - mean_x**2': 512},\n",
       "  'numerator': 512,\n",
       "  'denominator': 1024,\n",
       "  'gamma*(numerator/denominator)+beta': 786432},\n",
       " 'ProjectToVocab': 15728640000,\n",
       " 'Blocks': {'LayerNorms': {'mean(x)': 3151872,\n",
       "   'var_x': {'X**2': 3145728,\n",
       "    'sum': 3145728,\n",
       "    '/ m': 6144,\n",
       "    'mean_x**2': 6144,\n",
       "    'm - mean_x**2': 6144},\n",
       "   'numerator': 6144,\n",
       "   'denominator': 12288,\n",
       "   'gamma*(numerator/denominator)+beta': 9437184},\n",
       "  'ResidualConnections': 3145728,\n",
       "  'MultiHeadAttention': {'Q * WQ': 1610612736.0,\n",
       "   'K * WK': 1610612736.0,\n",
       "   'V * WV': 1610612736.0,\n",
       "   'ScaledDotProductAttention': {'Q * K.T': 1610612736.0,\n",
       "    '/ sqrt(d_k)': 12582912,\n",
       "    '+ mask': 12582912,\n",
       "    'Softmax': {'x - max(x)': 12582912, 'e^x': 12582912, 'sum': 12582912},\n",
       "    '* V': 1610612736.0},\n",
       "   '* WO': 1610612736.0},\n",
       "  'PositionWiseFFN': {'* W1': 12884901888,\n",
       "   '+ b1': 24576,\n",
       "   '* W2': 12884901888,\n",
       "   '+ b2': 3072}}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_dict(N, l, d_model, d_ff, n_vocab, n_layers, n_heads):\n",
    "    return Transformer(l, d_model, d_ff, n_vocab, n_layers, n_heads).flops_dict\n",
    "\n",
    "count_dict(\n",
    "    N=1,\n",
    "    l=512,\n",
    "    d_model=512,\n",
    "    d_ff=4096,\n",
    "    n_vocab=30000,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_map(d, f):\n",
    "    return {k: tree_map(v, f) if isinstance(v, dict) else f(v) for k, v in d.items()}\n",
    "\n",
    "def percent_flops(N,l,d_model,d_ff,n_vocab,n_layers,n_heads):\n",
    "    total_flops = count_flops(N, l, d_model, d_ff, n_vocab, n_layers, n_heads)\n",
    "    return tree_map(\n",
    "        d=count_dict(N, l, d_model, d_ff, n_vocab, n_layers, n_heads),\n",
    "        f=lambda x: x / total_flops\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Embeddings': {'X_word + X_pos': 5.115108268509576e-06},\n",
       " 'LayerNorm_Final': {'mean(x)': 5.125098714346509e-06,\n",
       "  'var_x': {'X**2': 5.115108268509576e-06,\n",
       "   'sum': 5.115108268509576e-06,\n",
       "   '/ m': 9.990445836932766e-09,\n",
       "   'mean_x**2': 9.990445836932766e-09,\n",
       "   'm - mean_x**2': 9.990445836932766e-09},\n",
       "  'numerator': 9.990445836932766e-09,\n",
       "  'denominator': 1.9980891673865532e-08,\n",
       "  'gamma*(numerator/denominator)+beta': 1.534532480552873e-05},\n",
       " 'ProjectToVocab': 0.3069064961105746,\n",
       " 'Blocks': {'LayerNorms': {'mean(x)': 6.150118457215811e-05,\n",
       "   'var_x': {'X**2': 6.138129922211491e-05,\n",
       "    'sum': 6.138129922211491e-05,\n",
       "    '/ m': 1.198853500431932e-07,\n",
       "    'mean_x**2': 1.198853500431932e-07,\n",
       "    'm - mean_x**2': 1.198853500431932e-07},\n",
       "   'numerator': 1.198853500431932e-07,\n",
       "   'denominator': 2.397707000863864e-07,\n",
       "   'gamma*(numerator/denominator)+beta': 0.00018414389766634474},\n",
       "  'ResidualConnections': 6.138129922211491e-05,\n",
       "  'MultiHeadAttention': {'Q * WQ': 0.031427225201722836,\n",
       "   'K * WK': 0.031427225201722836,\n",
       "   'V * WV': 0.031427225201722836,\n",
       "   'ScaledDotProductAttention': {'Q * K.T': 0.031427225201722836,\n",
       "    '/ sqrt(d_k)': 0.00024552519688845966,\n",
       "    '+ mask': 0.00024552519688845966,\n",
       "    'Softmax': {'x - max(x)': 0.00024552519688845966,\n",
       "     'e^x': 0.00024552519688845966,\n",
       "     'sum': 0.00024552519688845966},\n",
       "    '* V': 0.031427225201722836},\n",
       "   '* WO': 0.031427225201722836},\n",
       "  'PositionWiseFFN': {'* W1': 0.2514178016137827,\n",
       "   '+ b1': 4.795414001727728e-07,\n",
       "   '* W2': 0.2514178016137827,\n",
       "   '+ b2': 5.99426750215966e-08}}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_flops(\n",
    "    N=1,\n",
    "    l=512,\n",
    "    d_model=512,\n",
    "    d_ff=4096,\n",
    "    n_vocab=30000,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(N,l,d_model,d_ff,n_vocab,n_layers,n_heads, keys):\n",
    "    _count = lambda d: sum(_count(v) if isinstance(v, dict) else v for v in d.values())\n",
    "    def gather(d):\n",
    "        if not isinstance(d, dict):\n",
    "            return d\n",
    "        return {k: _count(v) if k in keys else gather(v) for k, v in d.items()}\n",
    "\n",
    "    flops = count_dict(N, l, d_model, d_ff, n_vocab, n_layers, n_heads)\n",
    "    total_flops = count_flops(N, l, d_model, d_ff, n_vocab, n_layers, n_heads)\n",
    "    \n",
    "    counts = gather(flops)\n",
    "    percentages = tree_map(\n",
    "        d=counts,\n",
    "        f=lambda x: x / total_flops\n",
    "    )\n",
    "    return counts, percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Embeddings': 7.566801465307276e-08,\n",
       " 'LayerNorm_Final': 4.542667188278953e-07,\n",
       " 'ProjectToVocab': 0.007748404700474651,\n",
       " 'Blocks': {'LayerNorms': 5.81461400099706e-05,\n",
       "  'ResidualConnections': 9.685505875593314e-06,\n",
       "  'MultiHeadAttention': {'Q * WQ': 0.07934366413286043,\n",
       "   'K * WK': 0.07934366413286043,\n",
       "   'V * WV': 0.07934366413286043,\n",
       "   'ScaledDotProductAttention': 0.04005925230145394,\n",
       "   '* WO': 0.07934366413286043},\n",
       "  'PositionWiseFFN': 0.6347493248860107}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis(\n",
    "    N=1,\n",
    "    l=2048,\n",
    "    d_model=8192,\n",
    "    d_ff=32768,\n",
    "    n_vocab=51200,\n",
    "    n_layers=64,\n",
    "    n_heads=64,\n",
    "    keys={\"Embeddings\", \"LayerNorm_Final\", \"LayerNorms\", \"PositionWiseFFN\", \"ScaledDotProductAttention\"}\n",
    ")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Embeddings': 1.119801735515862e-06,\n",
       " 'LayerNorm_Final': 6.722637860433361e-06,\n",
       " 'ProjectToVocab': 0.11466769771682427,\n",
       " 'Blocks': {'LayerNorms': 0.00032268661730080133,\n",
       "  'ResidualConnections': 5.375048330476138e-05,\n",
       "  'MultiHeadAttention': {'Q * WQ': 0.05504049490407565,\n",
       "   'K * WK': 0.05504049490407565,\n",
       "   'V * WV': 0.05504049490407565,\n",
       "   'ScaledDotProductAttention': 0.2244620182806835,\n",
       "   '* WO': 0.05504049490407565},\n",
       "  'PositionWiseFFN': 0.44032402484598815}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis(\n",
    "    N=1,\n",
    "    l=2048,\n",
    "    d_model=1024,\n",
    "    d_ff=4096,\n",
    "    n_vocab=51200,\n",
    "    n_layers=24,\n",
    "    n_heads=16,\n",
    "    keys={\"Embeddings\", \"LayerNorm_Final\", \"LayerNorms\", \"PositionWiseFFN\", \"ScaledDotProductAttention\"}\n",
    ")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2cb8452cdc519ce2845c8e2e168e9bb68d615667699426be3b874f0720b459a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
